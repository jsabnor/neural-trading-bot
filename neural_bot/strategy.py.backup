"""
Neural Trading Strategy - Sistema de trading con aprendizaje continuo

Implementa un modelo CNN-LSTM para detecci√≥n de patrones en gr√°ficos
con capacidad de aprendizaje continuo.

Uso:
    # Entrenamiento inicial
    python neural_strategy.py --mode train --symbols ETH/USDT BTC/USDT
    
    # Predicci√≥n
    python neural_strategy.py --mode predict --symbol ETH/USDT
    
    # Aprendizaje continuo
    python neural_strategy.py --mode continuous
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import json
import joblib
import warnings
warnings.filterwarnings('ignore')

# TensorFlow/Keras
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers, models
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
except ImportError:
    print("‚ö†Ô∏è TensorFlow no est√° instalado. Instala con: pip install tensorflow")
    print("   Para solo CPU: pip install tensorflow-cpu")
    exit(1)

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix

# Local imports
import sys
from pathlib import Path
# Add parent directory to path for DataCache
sys.path.insert(0, str(Path(__file__).parent.parent))
from data_cache import DataCache
from .config import config

from tensorflow.keras.callbacks import Callback
from sklearn.metrics import recall_score, f1_score


# MEJORA #3: Attention Layer con serializaci√≥n correcta
class AttentionLayer(layers.Layer):
    """
    Attention mechanism para enfocar en partes importantes de la secuencia temporal
    Compatible con TensorFlow 2.x save/load
    """
    def __init__(self, units=64, **kwargs):
        super().__init__(**kwargs)
        self.units = units
    
    def build(self, input_shape):
        self.W = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer='glorot_uniform',
            trainable=True,
            name='attention_W'
        )
        self.b = self.add_weight(
            shape=(self.units,),
            initializer='zeros',
            trainable=True,
            name='attention_b'
        )
        super().build(input_shape)
    
    def call(self, inputs):
        # Attention mechanism
        score = tf.nn.tanh(tf.matmul(inputs, self.W) + self.b)
        attention_weights = tf.nn.softmax(score, axis=1)
        context_vector = tf.reduce_sum(attention_weights * inputs, axis=1)
        return context_vector
    
    def get_config(self):
        config = super().get_config()
        config.update({"units": self.units})
        return config
    
    @classmethod
    def from_config(cls, config):
        return cls(**config)
import numpy as np

class BuyMetricsCallback(Callback):
    def __init__(self, X_val, y_val):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val

    def on_epoch_end(self, epoch, logs=None):
        y_pred = self.model.predict(self.X_val, verbose=0)
        y_pred_classes = np.argmax(y_pred, axis=1)
        
        # Recall y F1 de la clase BUY (2)
        recall_buy = recall_score(self.y_val, y_pred_classes, labels=[2], average='macro', zero_division=0)
        f1_buy = f1_score(self.y_val, y_pred_classes, labels=[2], average='macro', zero_division=0)
        
        print(f"\nüìä Epoch {epoch+1}: Recall BUY={recall_buy:.3f}, F1 BUY={f1_buy:.3f}")
        
        # Inject into logs for EarlyStopping/ReduceLROnPlateau
        if logs is not None:
            logs['val_f1_buy'] = f1_buy
            logs['val_recall_buy'] = recall_buy



@keras.utils.register_keras_serializable()
class FocalLoss(tf.keras.losses.Loss):
    """
    Focal Loss para manejar desbalance severo de clases
    
    Penaliza m√°s los errores en clases minoritarias (SELL, HOLD)
    y menos los aciertos en clases mayoritarias (BUY).
    """
    def __init__(self, gamma=2.0, alpha=0.25, name="focal_loss", **kwargs):
        super().__init__(name=name, **kwargs)
        self.gamma = gamma
        self.alpha = alpha

    def call(self, y_true, y_pred):
        # Convertir labels a one-hot encoding
        y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=config.NUM_CLASSES)
        
        # Clip predictions para estabilidad num√©rica
        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)
        
        # Cross entropy
        ce = -y_true * tf.math.log(y_pred)
        
        # Focal weight: (1 - p)^gamma
        # Ejemplos bien clasificados (p cercano a 1) tienen peso bajo
        # Ejemplos mal clasificados (p cercano a 0) tienen peso alto
        weight = tf.pow(1.0 - y_pred, self.gamma) * y_true
        
        # Focal loss con alpha balancing
        fl = self.alpha * weight * ce
        
        return tf.reduce_mean(tf.reduce_sum(fl, axis=-1))

    def get_config(self):
        config = super().get_config()
        config.update({
            "gamma": self.gamma,
            "alpha": self.alpha,
        })
        return config


class FeatureExtractor:
    """Extrae y normaliza features de datos OHLCV"""
    
    def __init__(self):
        self.scaler = MinMaxScaler()
        self.feature_names = []
        
    def calculate_technical_indicators(self, df):
        """Calcula indicadores t√©cnicos configurados (OPTIMIZADO)"""
        result = df.copy()
        
        # EMAs
        for name, period in config.TECHNICAL_INDICATORS.items():
            if 'ema' in name and name in ['ema_fast', 'ema_slow', 'ema_trend']:
                ema = result['close'].ewm(span=period, adjust=False).mean()
                result[name] = ema
                
                # NUEVO: Slope (Pendiente) de la EMA
                # Calcula el √°ngulo de la tendencia (en radianes o simplemente delta)
                # Usamos delta de 3 periodos normalizado por el precio
                result[f'{name}_slope'] = (ema.diff(3) / ema) * 100
        
        # RSI
        if 'rsi' in config.TECHNICAL_INDICATORS:
            period = config.TECHNICAL_INDICATORS['rsi']
            delta = result['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            result['rsi'] = 100 - (100 / (1 + rs))
        
        # ATR
        if 'atr' in config.TECHNICAL_INDICATORS:
            period = config.TECHNICAL_INDICATORS['atr']
            high_low = result['high'] - result['low']
            high_close = np.abs(result['high'] - result['close'].shift())
            low_close = np.abs(result['low'] - result['close'].shift())
            ranges = pd.concat([high_low, high_close, low_close], axis=1)
            true_range = ranges.max(axis=1)
            result['atr'] = true_range.rolling(window=period).mean()
        
        # ADX
        if 'adx' in config.TECHNICAL_INDICATORS:
            period = config.TECHNICAL_INDICATORS['adx']
            
            high_diff = result['high'].diff()
            low_diff = -result['low'].diff()
            
            plus_dm = high_diff.where((high_diff > low_diff) & (high_diff > 0), 0)
            minus_dm = low_diff.where((low_diff > high_diff) & (low_diff > 0), 0)
            
            if 'atr' not in result.columns:
                result['atr'] = true_range.rolling(window=period).mean()
            
            plus_di = 100 * (plus_dm.rolling(window=period).mean() / result['atr'])
            minus_di = 100 * (minus_dm.rolling(window=period).mean() / result['atr'])
            
            dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di)
            result['adx'] = dx.rolling(window=period).mean()
        
        # MACD (NUEVO)
        if all(k in config.TECHNICAL_INDICATORS for k in ['macd_fast', 'macd_slow', 'macd_signal']):
            fast = config.TECHNICAL_INDICATORS['macd_fast']
            slow = config.TECHNICAL_INDICATORS['macd_slow']
            signal = config.TECHNICAL_INDICATORS['macd_signal']
            
            ema_fast = result['close'].ewm(span=fast, adjust=False).mean()
            ema_slow = result['close'].ewm(span=slow, adjust=False).mean()
            
            result['macd_line'] = ema_fast - ema_slow
            result['macd_signal'] = result['macd_line'].ewm(span=signal, adjust=False).mean()
            result['macd_histogram'] = result['macd_line'] - result['macd_signal']
        
        # Bollinger Bands (NUEVO)
        if 'bb_period' in config.TECHNICAL_INDICATORS and 'bb_std' in config.TECHNICAL_INDICATORS:
            period = config.TECHNICAL_INDICATORS['bb_period']
            std_dev = config.TECHNICAL_INDICATORS['bb_std']
            
            result['bb_middle'] = result['close'].rolling(window=period).mean()
            bb_std = result['close'].rolling(window=period).std()
            
            result['bb_upper'] = result['bb_middle'] + (bb_std * std_dev)
            result['bb_lower'] = result['bb_middle'] - (bb_std * std_dev)
            
            # %B: Posici√≥n relativa dentro de las bandas
            result['bb_percent'] = (result['close'] - result['bb_lower']) / (result['bb_upper'] - result['bb_lower'])
            
            # Bandwidth: Ancho de las bandas (volatilidad)
            result['bb_bandwidth'] = (result['bb_upper'] - result['bb_lower']) / result['bb_middle']
        
        # Stochastic Oscillator (NUEVO)
        if 'stoch_k' in config.TECHNICAL_INDICATORS and 'stoch_d' in config.TECHNICAL_INDICATORS:
            k_period = config.TECHNICAL_INDICATORS['stoch_k']
            d_period = config.TECHNICAL_INDICATORS['stoch_d']
            
            low_min = result['low'].rolling(window=k_period).min()
            high_max = result['high'].rolling(window=k_period).max()
            
            result['stoch_k'] = 100 * (result['close'] - low_min) / (high_max - low_min)
            result['stoch_d'] = result['stoch_k'].rolling(window=d_period).mean()
        
        # CCI - Commodity Channel Index (NUEVO)
        if 'cci' in config.TECHNICAL_INDICATORS:
            period = config.TECHNICAL_INDICATORS['cci']
            
            tp = (result['high'] + result['low'] + result['close']) / 3
            sma_tp = tp.rolling(window=period).mean()
            mad = tp.rolling(window=period).apply(lambda x: np.abs(x - x.mean()).mean())
            
            result['cci'] = (tp - sma_tp) / (0.015 * mad)
        
        # VWAP - Volume Weighted Average Price (NUEVO)
        if hasattr(config, 'VOLUME_FEATURES') and 'vwap' in config.VOLUME_FEATURES:
            typical_price = (result['high'] + result['low'] + result['close']) / 3
            result['vwap'] = (typical_price * result['volume']).cumsum() / result['volume'].cumsum()
        
        # OBV - On Balance Volume (NUEVO)
        if hasattr(config, 'VOLUME_FEATURES') and 'obv' in config.VOLUME_FEATURES:
            obv = [0]
            for i in range(1, len(result)):
                if result['close'].iloc[i] > result['close'].iloc[i-1]:
                    obv.append(obv[-1] + result['volume'].iloc[i])
                elif result['close'].iloc[i] < result['close'].iloc[i-1]:
                    obv.append(obv[-1] - result['volume'].iloc[i])
                else:
                    obv.append(obv[-1])
            result['obv'] = obv
        
        # Volume Ratio (NUEVO)
        if hasattr(config, 'VOLUME_FEATURES') and 'volume_ratio' in config.VOLUME_FEATURES:
            result['volume_ratio'] = result['volume'] / result['volume'].rolling(window=20).mean()
        
        return result

    
    def calculate_price_features(self, df):
        """Calcula features basadas en precio (OPTIMIZADO)"""
        result = df.copy()
        
        # Returns
        if 'returns' in config.PRICE_FEATURES:
            result['returns'] = result['close'].pct_change()
        
        # Log returns
        if 'log_returns' in config.PRICE_FEATURES:
            result['log_returns'] = np.log(result['close'] / result['close'].shift(1))
        
        # Volatilidad
        if 'volatility' in config.PRICE_FEATURES:
            result['volatility'] = result['close'].pct_change().rolling(window=20).std()
        
        # High-Low ratio
        if 'hl_ratio' in config.PRICE_FEATURES:
            result['hl_ratio'] = (result['high'] - result['low']) / result['close']
        
        # Open-Close ratio
        if 'oc_ratio' in config.PRICE_FEATURES:
            result['oc_ratio'] = (result['close'] - result['open']) / result['open']
        
        # Volume change
        if 'volume_change' in config.PRICE_FEATURES:
            result['volume_change'] = result['volume'].pct_change()
        
        # CROSS FEATURES (NUEVO)
        if hasattr(config, 'CROSS_FEATURES'):
            # EMA Crossover Signal
            if 'ema_cross' in config.CROSS_FEATURES and 'ema_fast' in result.columns and 'ema_slow' in result.columns:
                result['ema_cross'] = (result['ema_fast'] - result['ema_slow']) / result['close']
            
            # Distance from price to EMA fast
            if 'price_to_ema_fast' in config.CROSS_FEATURES and 'ema_fast' in result.columns:
                result['price_to_ema_fast'] = (result['close'] - result['ema_fast']) / result['close']
            
            # Distance from price to EMA slow
            if 'price_to_ema_slow' in config.CROSS_FEATURES and 'ema_slow' in result.columns:
                result['price_to_ema_slow'] = (result['close'] - result['ema_slow']) / result['close']
                
            # NUEVO: Distancia a EMA 200 (Trend)
            if 'ema_trend' in result.columns:
                result['dist_to_trend'] = (result['close'] - result['ema_trend']) / result['ema_trend']
        
        return result
    
    def calculate_market_regime(self, df):
        """
        Calcula features de r√©gimen de mercado para mejor adaptaci√≥n a condiciones
        
        Returns:
            DataFrame con features de r√©gimen a√±adidas
        """
        result = df.copy()
        
        # 1. TREND DIRECTION: Bull (1) vs Bear (0)
        if 'ema_trend' in result.columns:
            result['trend_direction'] = (result['close'] > result['ema_trend']).astype(float)
        
        # 2. VOLATILITY REGIME: Normalizado 0-1
        if 'atr' in result.columns:
            # Volatilidad relativa
            vol_raw = result['atr'] / result['close']
            
            # Normalizar usando rolling min/max (50 per√≠odos)
            vol_min = vol_raw.rolling(window=50, min_periods=1).min()
            vol_max = vol_raw.rolling(window=50, min_periods=1).max()
            
            # Evitar divisi√≥n por cero
            vol_range = vol_max - vol_min
            vol_range = vol_range.replace(0, 1)
            
            result['volatility_regime'] = (vol_raw - vol_min) / vol_range
            result['volatility_regime'] = result['volatility_regime'].fillna(0.5)  # Neutral si no hay datos
        
        # 3. TREND STRENGTH: Trending (1) vs Lateral (0)
        if 'adx' in result.columns:
            result['trend_strength'] = (result['adx'] > 25).astype(float)
        
        return result

    
    def extract_features(self, df, fit_scaler=False):
        """
        Extrae todas las features de un DataFrame OHLCV
        
        Args:
            df: DataFrame con columnas [timestamp, open, high, low, close, volume]
            fit_scaler: Si True, ajusta el scaler (solo para entrenamiento)
        
        Returns:
            np.array con features normalizadas, shape (n_samples, n_features)
        """
        # Calcular indicadores
        df_features = self.calculate_technical_indicators(df)
        df_features = self.calculate_price_features(df_features)
        df_features = self.calculate_market_regime(df_features)  # NUEVO: R√©gimen de mercado
        
        # Seleccionar columnas de features
        feature_cols = []

        # OHLCV b√°sicos
        # feature_cols.extend(['open', 'high', 'low', 'close', 'volume']) # REMOVED: Non-stationary features
        # Solo usamos variaciones (returns, vol_change) ya incluidas en PRICE_FEATURES

        # Indicadores t√©cnicos base
        for name in ['ema_fast', 'ema_slow', 'ema_trend', 'rsi', 'atr', 'adx']:
            if name in df_features.columns:
                feature_cols.append(name)
            # Agregar slopes si existen
            if f'{name}_slope' in df_features.columns:
                feature_cols.append(f'{name}_slope')

        # MACD features
        for macd_feat in ['macd_line', 'macd_signal', 'macd_histogram']:
            if macd_feat in df_features.columns:
                feature_cols.append(macd_feat)

        # Bollinger Bands features  
        for bb_feat in ['bb_percent', 'bb_bandwidth']:
            if bb_feat in df_features.columns:
                feature_cols.append(bb_feat)

        # Stochastic features
        for stoch_feat in ['stoch_k', 'stoch_d']:
            if stoch_feat in df_features.columns:
                feature_cols.append(stoch_feat)

        # CCI
        if 'cci' in df_features.columns:
            feature_cols.append('cci')

        # Features de precio
        for name in config.PRICE_FEATURES:
            if name in df_features.columns:
                feature_cols.append(name)

        # Volume features (NUEVO)
        if hasattr(config, 'VOLUME_FEATURES'):
            for name in config.VOLUME_FEATURES:
                if name in df_features.columns:
                    feature_cols.append(name)

        # Cross features (NUEVO)
        if hasattr(config, 'CROSS_FEATURES'):
            for name in config.CROSS_FEATURES:
                if name in df_features.columns:
                    feature_cols.append(name)
            
            # Agregar dist_to_trend si existe
            if 'dist_to_trend' in df_features.columns:
                feature_cols.append('dist_to_trend')

        # Market regime features (NUEVO - MEJORA #1)
        if hasattr(config, 'MARKET_REGIME_FEATURES'):
            for name in config.MARKET_REGIME_FEATURES:
                if name in df_features.columns:
                    feature_cols.append(name)

        # Guardar nombres de features
        self.feature_names = feature_cols
        
        # Extraer features
        df_temp = df_features[feature_cols].copy()
        
        # CR√çTICO: Manejar valores infinitos y NaN ANTES de normalizar
        # 1. Reemplazar infinitos con NaN
        df_temp = df_temp.replace([np.inf, -np.inf], np.nan)
        
        # 2. Rellenar NaN con forward fill, backward fill, y finalmente 0
        df_temp = df_temp.fillna(method='ffill').fillna(method='bfill').fillna(0)
        
        # 3. Convertir a numpy
        X = df_temp.values
        
        # 4. Verificar que no queden NaN o infinitos
        if np.any(np.isnan(X)) or np.any(np.isinf(X)):
            print(f"‚ö†Ô∏è Advertencia: A√∫n hay NaN o infinitos despu√©s de limpieza")
            # √öltimo recurso: reemplazar con 0
            X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        
        # Normalizar
        if fit_scaler:
            X = self.scaler.fit_transform(X)
        else:
            X = self.scaler.transform(X)
        
        return X
    
    def create_sequences(self, X, y=None):
        """
        Crea secuencias de ventanas temporales para LSTM
        
        Args:
            X: Features normalizadas (n_samples, n_features)
            y: Labels opcionales (n_samples,)
        
        Returns:
            X_seq: (n_sequences, lookback, n_features)
            y_seq: (n_sequences,) si y fue proporcionado
        """
        lookback = config.LOOKBACK_WINDOW
        
        X_seq = []
        y_seq = [] if y is not None else None
        
        for i in range(lookback, len(X)):
            X_seq.append(X[i-lookback:i])
            if y is not None:
                y_seq.append(y[i])
        
        X_seq = np.array(X_seq)
        
        if y is not None:
            y_seq = np.array(y_seq)
            return X_seq, y_seq
        
        return X_seq
    
    def save_scaler(self, version):
        """Guarda el scaler entrenado"""
        path = Path(config.MODELS_DIR) / config.CONFIG_NAME_FORMAT.format(version=version)
        joblib.dump(self.scaler, path)
        print(f"üíæ Scaler guardado: {path}")
    
    def load_scaler(self, version):
        """Carga un scaler guardado"""
        path = Path(config.MODELS_DIR) / config.CONFIG_NAME_FORMAT.format(version=version)
        if path.exists():
            self.scaler = joblib.load(path)
            print(f"üìÇ Scaler cargado: {path}")
            return True
        return False


class DataLabeler:
    """Etiqueta datos hist√≥ricos para entrenamiento supervisado"""
    
    @staticmethod
    def label_data(df):
        """
        Etiqueta datos usando UMBRALES FIJOS (NUEVA ESTRATEGIA)
        
        Soporta dos modos:
        - Binario: BUY vs NO_BUY (m√°s simple, mejor para aprendizaje)
        - Ternario: SELL vs HOLD vs BUY (m√°s complejo)
        
        Ventajas sobre percentiles:
        - Consistente entre s√≠mbolos
        - Sim√©trico (BUY y SELL tratados igual)
        - F√°cil de entender y ajustar
        - No depende de la distribuci√≥n de datos
        
        Returns:
            np.array de labels: 
            - Binario: 0=NO_BUY, 1=BUY
            - Ternario: 0=SELL, 1=HOLD, 2=BUY
        """
        lookahead = config.LABEL_LOOKAHEAD
        binary_mode = config.USE_BINARY_CLASSIFICATION
        
        # Usar umbrales fijos, din√°micos ATR, o percentiles seg√∫n configuraci√≥n
        if hasattr(config, 'USE_DYNAMIC_ATR_THRESHOLDS') and config.USE_DYNAMIC_ATR_THRESHOLDS:
            # MEJORA #2: Umbrales din√°micos basados en ATR
            print(f"üìä Etiquetado con UMBRALES DIN√ÅMICOS ATR:")
            print(f"   ATR Multiplier BUY:  {config.ATR_MULTIPLIER_BUY}x")
            print(f"   ATR Multiplier SELL: {config.ATR_MULTIPLIER_SELL}x")
            print(f"   ATR Period: {config.ATR_PERIOD}")
            
            # Calcular ATR si no existe
            if 'atr' not in df.columns:
                period = config.ATR_PERIOD
                high_low = df['high'] - df['low']
                high_close = np.abs(df['high'] - df['close'].shift())
                low_close = np.abs(df['low'] - df['close'].shift())
                ranges = pd.concat([high_low, high_close, low_close], axis=1)
                true_range = ranges.max(axis=1)
                df['atr'] = true_range.rolling(window=period).mean()
            
            # Los umbrales ser√°n calculados por vela (din√°micos)
            buy_threshold = None  # Se calcula en el loop
            sell_threshold = None  # Se calcula en el loop
            use_dynamic_atr = True
            
        elif config.USE_FIXED_THRESHOLDS:
            # ESTRATEGIA NUEVA: Umbrales fijos
            buy_threshold = config.LABEL_BUY_THRESHOLD    # +2.0%
            sell_threshold = config.LABEL_SELL_THRESHOLD  # -2.0%
            use_dynamic_atr = False
            
            if binary_mode:
                print(f"üìä Etiquetado BINARIO con umbrales fijos:")
                print(f"   BUY:    retorno >= {buy_threshold*100:+.1f}%")
                print(f"   NO_BUY: retorno < {buy_threshold*100:+.1f}%")
            else:
                print(f"üìä Etiquetado TERNARIO con umbrales fijos:")
                print(f"   BUY:  retorno >= {buy_threshold*100:+.1f}%")
                print(f"   SELL: retorno <= {sell_threshold*100:+.1f}%")
                print(f"   HOLD: entre {sell_threshold*100:+.1f}% y {buy_threshold*100:+.1f}%")
            
        else:
            # ESTRATEGIA ANTIGUA: Percentiles din√°micos (DEPRECADA)
            print("‚ö†Ô∏è Usando estrategia de percentiles (DEPRECADA)")
            min_movement = 0.010
            
        labels = []
        
        # Calcular etiquetas
        for i in range(len(df)):
            # √öltimas velas no tienen futuro suficiente
            if i >= len(df) - lookahead:
                labels.append(1 if binary_mode else 1)  # NO_BUY o HOLD
                continue
            
            current_price = df.iloc[i]['close']
            
            if hasattr(config, 'USE_DYNAMIC_ATR_THRESHOLDS') and config.USE_DYNAMIC_ATR_THRESHOLDS:
                # MEJORA #2: Umbrales din√°micos basados en ATR
                future_price = df.iloc[i + lookahead]['close']
                return_pct = (future_price - current_price) / current_price
                
                # Calcular umbrales din√°micos para esta vela
                        labels.append(0)  # NO_BUY
                else:
                    # MODO TERNARIO: SELL/HOLD/BUY
                    if return_pct >= buy_threshold:
                        labels.append(2)  # BUY
                    elif return_pct <= sell_threshold:
                        labels.append(0)  # SELL
                    else:
                        labels.append(1)  # HOLD
                    
            else:
                # ANTIGUA ESTRATEGIA: Percentiles (mantener por compatibilidad)
                future_prices = df.iloc[i+1:i+lookahead+1]['close']
                max_gain = (future_prices.max() - current_price) / current_price
                max_loss = (future_prices.min() - current_price) / current_price
                
                if abs(max_gain) > abs(max_loss):
                    future_return = max_gain
                else:
                    future_return = max_loss
                
                # Usar percentiles si hay suficientes datos
                if abs(future_return) < min_movement:
                    labels.append(0 if binary_mode else 1)  # NO_BUY o HOLD
                elif future_return >= min_movement:
                    labels.append(1 if binary_mode else 2)  # BUY
                elif future_return <= -min_movement:
                    labels.append(0)  # NO_BUY o SELL
                else:
                    labels.append(0 if binary_mode else 1)  # NO_BUY o HOLD
        
        labels_array = np.array(labels)
        
        # Mostrar estad√≠sticas de etiquetado
        from collections import Counter
        label_counts = Counter(labels_array)
        total = len(labels_array)
        
        print(f"\nüìä Distribuci√≥n de etiquetas generadas:")
        if binary_mode:
            print(f"   NO_BUY (0): {label_counts[0]:5d} ({label_counts[0]/total*100:5.1f}%)")
            print(f"   BUY    (1): {label_counts[1]:5d} ({label_counts[1]/total*100:5.1f}%)")
        else:
            print(f"   SELL (0): {label_counts[0]:5d} ({label_counts[0]/total*100:5.1f}%)")
            print(f"   HOLD (1): {label_counts[1]:5d} ({label_counts[1]/total*100:5.1f}%)")
            print(f"   BUY  (2): {label_counts[2]:5d} ({label_counts[2]/total*100:5.1f}%)")
        print(f"   Total:    {total:5d}")
        
        return labels_array



class NeuralTradingModel:
    """Modelo CNN-LSTM para predicci√≥n de se√±ales de trading"""
    
    def __init__(self, input_shape):
        """
        Args:
            input_shape: (lookback_window, n_features)
        """
        self.input_shape = input_shape
        self.model = None
        self.build_model()
    
    def build_model(self):
        """Construye arquitectura CNN-LSTM h√≠brida con Attention (OPTIMIZADO)"""
        
        # Input
        inputs = layers.Input(shape=self.input_shape)
        
        # CNN 1D para detectar patrones locales
        x = inputs
        for filters in config.CNN_FILTERS:
            x = layers.Conv1D(
                filters=filters,
                kernel_size=config.CNN_KERNEL_SIZE,
                padding='same',
                activation='relu'
            )(x)
            x = layers.BatchNormalization()(x)
            x = layers.MaxPooling1D(pool_size=2)(x)
        
        # LSTM para dependencias temporales
        if config.USE_ATTENTION:
            # Con Attention: necesita return_sequences=True
            lstm_out = layers.LSTM(
                units=config.LSTM_UNITS,
                dropout=config.LSTM_DROPOUT,
                return_sequences=True
            )(x)
            
            # BatchNormalization despu√©s de LSTM
            lstm_out = layers.BatchNormalization()(lstm_out)
            
            # Attention Mechanism
            attention = layers.Dense(1, activation='tanh')(lstm_out)
            attention = layers.Flatten()(attention)
            attention = layers.Activation('softmax')(attention)
            attention = layers.RepeatVector(config.LSTM_UNITS)(attention)
            attention = layers.Permute([2, 1])(attention)
            
            # Apply attention weights
            x = layers.Multiply()([lstm_out, attention])
            x = layers.Lambda(lambda xin: tf.reduce_sum(xin, axis=1))(x)
        else:
            # Sin Attention: LSTM normal
            x = layers.LSTM(
                units=config.LSTM_UNITS,
                dropout=config.LSTM_DROPOUT,
                return_sequences=False
            )(x)
            x = layers.BatchNormalization()(x)
        
        # Capas densas
        for units in config.DENSE_UNITS:
            x = layers.Dense(units, activation='relu')(x)
            x = layers.Dropout(config.DENSE_DROPOUT)(x)
        
        # Output: 3 clases (SELL, HOLD, BUY)
        outputs = layers.Dense(config.NUM_CLASSES, activation='softmax')(x)
        
        # Compilar modelo
        self.model = models.Model(inputs=inputs, outputs=outputs)
        
        # Seleccionar loss function
        if config.USE_FOCAL_LOSS:
            loss_fn = FocalLoss(gamma=config.FOCAL_LOSS_GAMMA, alpha=config.FOCAL_LOSS_ALPHA)
            loss_name = f"Focal Loss (Œ≥={config.FOCAL_LOSS_GAMMA}, Œ±={config.FOCAL_LOSS_ALPHA})"
        else:
            loss_fn = config.LOSS_FUNCTION
            loss_name = config.LOSS_FUNCTION
        
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),
            loss=loss_fn,
            metrics=config.METRICS
        )
        
        print("‚úÖ Modelo construido (OPTIMIZADO):")
        print(f"   Input shape: {self.input_shape}")
        print(f"   CNN filters: {config.CNN_FILTERS}")
        print(f"   LSTM units: {config.LSTM_UNITS}")
        print(f"   Attention: {'Enabled' if config.USE_ATTENTION else 'Disabled'}")
        print(f"   Dense layers: {config.DENSE_UNITS}")
        print(f"   Loss function: {loss_name}")
        print(f"   Par√°metros: {self.model.count_params():,}")
    
    def get_summary(self):
        """Muestra resumen del modelo"""
        return self.model.summary()
    
    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=None, class_weights=None):
        """
        Entrena el modelo
        
        Args:
            X_train: Features de entrenamiento (n_samples, lookback, n_features)
            y_train: Labels de entrenamiento (n_samples,)
            X_val: Features de validaci√≥n (opcional)
            y_val: Labels de validaci√≥n (opcional)
            epochs: N√∫mero de √©pocas (usa config si no se especifica)
            class_weights: Diccionario de pesos de clases (opcional, usa config si None)
        
        Returns:
            history: Historial de entrenamiento
        """
        if epochs is None:
            epochs = config.INITIAL_EPOCHS
        
        # Usar class weights proporcionados o configurados
        if class_weights is None:
            class_weight_dict = config.CLASS_WEIGHTS.copy()
            print(f"\n‚öñÔ∏è Class Weights (desde config):")
        else:
            class_weight_dict = class_weights
            print(f"\n‚öñÔ∏è Class Weights (calculados autom√°ticamente):")
            
        for cls, weight in class_weight_dict.items():
            print(f"   {config.CLASS_LABELS[cls]}: {weight:.2f}x")
        
        # Callbacks
        callbacks = [
            EarlyStopping(
                monitor='val_f1_buy' if X_val is not None else 'loss',
                mode='max' if X_val is not None else 'min',
                patience=config.EARLY_STOPPING_PATIENCE,
                restore_best_weights=True,
                verbose=1
            )
        ]

        # Learning Rate Schedule (NUEVO)
        if config.USE_LR_SCHEDULE:
            from tensorflow.keras.callbacks import ReduceLROnPlateau
            lr_scheduler = ReduceLROnPlateau(
                monitor='val_f1_buy' if X_val is not None else 'loss',
                mode='max' if X_val is not None else 'min',
                factor=config.LR_FACTOR,
                patience=config.LR_PATIENCE,
                min_lr=config.LR_MIN,
                verbose=1
            )
            callbacks.append(lr_scheduler)
            print(f"\nüìâ Learning Rate Schedule activado:")
            print(f"   Initial LR: {config.LEARNING_RATE}")
            print(f"   Factor: {config.LR_FACTOR}")
            print(f"   Patience: {config.LR_PATIENCE}")
            print(f"   Min LR: {config.LR_MIN}")
        
        # Validaci√≥n
        validation_data = None
        if X_val is not None and y_val is not None:
            validation_data = (X_val, y_val)
        
        # Entrenar
        print(f"\nüéì Entrenando modelo...")
        print(f"   Samples: {len(X_train)}")
        print(f"   Epochs: {epochs}")
        print(f"   Batch size: {config.BATCH_SIZE}")

        # A√±adir tu callback personalizado a la lista existente
        callbacks.append(BuyMetricsCallback(X_val, y_val))

        history = self.model.fit(
            X_train, y_train,
            validation_data=validation_data,
            epochs=epochs,
            batch_size=config.BATCH_SIZE,
            callbacks=callbacks,
            class_weight=class_weight_dict,  # CR√çTICO: Aplicar pesos
            verbose=config.VERBOSE
        )
        
        return history
    
    def predict(self, X):
        """
        Predice se√±ales
        
        Args:
            X: Features (n_samples, lookback, n_features)
        
        Returns:
            predictions: Array de probabilidades (n_samples, 3)
        """
        return self.model.predict(X, verbose=0)
    
    def predict_signal(self, X):
        """
        Predice se√±al con etiqueta
        
        Args:
            X: Features (1, lookback, n_features) o (lookback, n_features)
        
        Returns:
            dict: {'signal': 'BUY'/'SELL'/'HOLD', 'confidence': float, 'probabilities': dict}
        """
        # Asegurar shape correcto
        if len(X.shape) == 2:
            X = np.expand_dims(X, axis=0)
        
        # Predicci√≥n
        probs = self.predict(X)[0]
        
        # Clase con mayor probabilidad
        predicted_class = np.argmax(probs)
        confidence = probs[predicted_class]
        
        # Aplicar umbrales de confianza
        signal = config.CLASS_LABELS[predicted_class]
        
        if signal == 'BUY' and confidence < config.MIN_CONFIDENCE_BUY:
            signal = 'HOLD'
        elif signal == 'SELL' and confidence < config.MIN_CONFIDENCE_SELL:
            signal = 'HOLD'
        
        return {
            'signal': signal,
            'confidence': float(confidence),
            'probabilities': {
                'SELL': float(probs[0]),
                'HOLD': float(probs[1]),
                'BUY': float(probs[2])
            }
        }
    
    def evaluate(self, X_test, y_test):
        """Eval√∫a el modelo en datos de test"""
        print("\nüìä Evaluando modelo...")
        
        # Predicciones
        y_pred_probs = self.predict(X_test)
        y_pred = np.argmax(y_pred_probs, axis=1)
        
        # Definir nombres de clases seg√∫n modo
        if config.USE_BINARY_CLASSIFICATION:
            target_names = ['NO_BUY', 'BUY']
        else:
            target_names = ['SELL', 'HOLD', 'BUY']
        
        # M√©tricas
        print("\nClassification Report:")
        print(classification_report(
            y_test, y_pred,
            target_names=target_names
        ))
        
        print("\nConfusion Matrix:")
        print(confusion_matrix(y_test, y_pred))
        
        # Accuracy
        accuracy = np.mean(y_pred == y_test)
        print(f"\n‚úÖ Accuracy: {accuracy:.4f}")
        
        return accuracy
    
    def save(self, version):
        """Guarda el modelo"""
        path = Path(config.MODELS_DIR) / config.MODEL_NAME_FORMAT.format(version=version)
        self.model.save(path)
        print(f"üíæ Modelo guardado: {path}")
    
    def load(self, version):
        """Carga un modelo guardado"""
        path = Path(config.MODELS_DIR) / config.MODEL_NAME_FORMAT.format(version=version)
        if path.exists():
            self.model = keras.models.load_model(path)
            print(f"üìÇ Modelo cargado: {path}")
            return True
        return False


class ContinuousLearner:
    """Gestiona el ciclo de aprendizaje continuo"""
    
    def __init__(self):
        self.cache = DataCache()
        self.feature_extractor = FeatureExtractor()
        self.model = None
        self.current_version = 0
        self.metrics_history = []
    
    def get_latest_version(self):
        """Encuentra la versi√≥n m√°s reciente del modelo"""
        models_dir = Path(config.MODELS_DIR)
        if not models_dir.exists():
            return 0
        
        model_files = list(models_dir.glob('neural_model_v*.keras'))
        if not model_files:
            return 0
        
        versions = []
        for f in model_files:
            try:
                version = int(f.stem.split('_v')[1])
                versions.append(version)
            except:
                continue
        
        return max(versions) if versions else 0
    
    def load_latest_model(self):
        """Carga el modelo m√°s reciente"""
        version = self.get_latest_version()
        if version == 0:
            print("‚ö†Ô∏è No hay modelos guardados")
            return False
        
        self.current_version = version
        
        # Cargar scaler
        if not self.feature_extractor.load_scaler(version):
            print(f"‚ùå No se pudo cargar scaler v{version}")
            return False
        
        X_train, y_train, X_val, y_val = self.prepare_training_data(symbols, timeframe)
        
        # Crear modelo
        input_shape = (X_train.shape[1], X_train.shape[2])  # (lookback, n_features)
        self.model = NeuralTradingModel(input_shape)
        
        # Mostrar resumen
        self.model.get_summary()
        
        # Entrenar
        history = self.model.train(X_train, y_train, X_val, y_val)
        
        # Evaluar
        accuracy = self.model.evaluate(X_val, y_val)
        
        # Guardar
        self.current_version += 1
        self.model.save(self.current_version)
        self.feature_extractor.save_scaler(self.current_version)
        
        # Guardar m√©tricas
        self.save_metrics(self.current_version, {
            'accuracy': accuracy,
            'train_samples': len(X_train),
            'val_samples': len(X_val),
            'timestamp': datetime.now().isoformat(),
            'symbols': symbols or config.DEFAULT_SYMBOLS,
            'timeframe': timeframe
        })
        
        print(f"\n‚úÖ Modelo v{self.current_version} entrenado y guardado")
        
        return self.model
    
    def prepare_training_data(self, symbols=None, timeframe='4h', start_date=None, end_date=None):
        """
        Prepara datos de entrenamiento BALANCEADOS y con SPLIT CORRECTO
        """
        import numpy as np
        import pandas as pd

        if symbols is None:
            symbols = config.DEFAULT_SYMBOLS

        print(f"\nüìä Preparando datos de entrenamiento BALANCEADOS...")
        print(f"   S√≠mbolos: {symbols}")

        symbol_data = {}
        min_samples = float('inf')

        # 1. Cargar y procesar todos los s√≠mbolos
        for symbol in symbols:
            print(f"\n  Procesando {symbol}...")

            df = self.cache.get_data(symbol, timeframe)
            if df is None or len(df) < config.MIN_TRAIN_SAMPLES:
                print(f"    ‚ö†Ô∏è Datos insuficientes")
                continue

            # Filtrar por fechas si se especifican
            if start_date:
                start_dt = pd.to_datetime(start_date)
                df = df[df['timestamp'] >= start_dt]
                print(f"    üìÖ Filtrando desde: {start_date}")
            if end_date:
                end_dt = pd.to_datetime(end_date)
                df = df[df['timestamp'] <= end_dt]
                print(f"    üìÖ Filtrando hasta: {end_date}")

            # Generar etiquetas si no existen
            if 'label' not in df.columns:
                df['label'] = DataLabeler.label_data(df)

            # FILTRADO DE TENDENCIAS (NUEVO)
            if config.FILTER_LATERAL_MARKETS:
                print(f"    üîç Filtrando mercados laterales (ADX < {config.MIN_ADX_FOR_TRAINING})...")
                original_len = len(df)
                
                # Calcular ADX si no existe
                if 'adx' not in df.columns:
                    df = self.feature_extractor.calculate_technical_indicators(df)
                
                # Separar datos por r√©gimen de mercado
                df_trend = df[df['adx'] >= config.MIN_ADX_FOR_TRAINING].copy()
                df_lateral = df[df['adx'] < config.MIN_ADX_FOR_TRAINING].copy()
                
                # Mantener solo un porcentaje de datos laterales
                lateral_to_keep = int(len(df_lateral) * config.LATERAL_DATA_RATIO)
                if lateral_to_keep > 0:
                    df_lateral = df_lateral.sample(n=lateral_to_keep, random_state=config.RANDOM_SEED)
                
                # Combinar y reordenar por timestamp
                df = pd.concat([df_trend, df_lateral]).sort_values('timestamp').reset_index(drop=True)
                
                removed = original_len - len(df)
                print(f"    üìä Filtrado: {len(df_trend)} tendencias + {len(df_lateral)} laterales = {len(df)} velas")
                print(f"    üóëÔ∏è Eliminadas: {removed} velas laterales ({removed/original_len*100:.1f}%)")

            # Extraer features y labels
            y = df['label'].values
            
            # Usar FeatureExtractor para procesar X correctamente
            # fit_scaler=True para ajustar el escalador con los datos de entrenamiento
            X = self.feature_extractor.extract_features(df, fit_scaler=True)

            print(f"   üîç Features extra√≠das: {self.feature_extractor.feature_names}")


            # Crear secuencias
            X_seq, y_seq = self.feature_extractor.create_sequences(X, y)

            print(f"    ‚úÖ {len(X_seq)} secuencias generadas")

            symbol_data[symbol] = (X_seq, y_seq)
            min_samples = min(min_samples, len(X_seq))

        if not symbol_data:
            raise ValueError("No se pudieron cargar datos de ning√∫n s√≠mbolo")

        print(f"\n‚öñÔ∏è Balanceando a {min_samples} muestras por par (undersampling)")

        X_train_list, y_train_list, X_val_list, y_val_list = [], [], [], []

        # 2. Balancear y Split por s√≠mbolo
        for symbol, (X, y) in symbol_data.items():
            X_bal = X[-min_samples:]
            y_bal = y[-min_samples:]

            split_idx = int(len(X_bal) * (1 - config.VALIDATION_SPLIT))

            X_train_sym, y_train_sym = X_bal[:split_idx], y_bal[:split_idx]
            X_val_sym, y_val_sym = X_bal[split_idx:], y_bal[split_idx:]

            X_train_list.append(X_train_sym)
            y_train_list.append(y_train_sym)
            X_val_list.append(X_val_sym)
            y_val_list.append(y_val_sym)

            print(f"  {symbol}: Train {len(X_train_sym)} | Val {len(X_val_sym)}")

        # 3. Concatenar
        X_train = np.concatenate(X_train_list, axis=0)
        y_train = np.concatenate(y_train_list, axis=0)
        X_val = np.concatenate(X_val_list, axis=0)
        y_val = np.concatenate(y_val_list, axis=0)

        # 4. Shuffle (Solo Train)
        indices = np.arange(len(X_train))
        np.random.shuffle(indices)
        X_train, y_train = X_train[indices], y_train[indices]
        # 5. Oversampling de BUY (para evitar sesgo)
        # DESACTIVADO: Ya usamos Class Weights agresivos. Usar ambos causa overfitting extremo.
        # if not config.USE_BINARY_CLASSIFICATION and config.NUM_CLASSES == 3:
        #     idx_buy = np.where(y_train == 2)[0]
        #     idx_other = np.where(y_train != 2)[0]

        #     # Duplicar BUY hasta igualar otras clases
        #     oversampled_buy = np.random.choice(idx_buy, size=len(idx_other)//2, replace=True)

        #     final_idx = np.concatenate([idx_other, oversampled_buy])
        #     np.random.shuffle(final_idx)

        #     X_train = X_train[final_idx]
        #     y_train = y_train[final_idx]

        #     print(f"\nüìä Oversampling aplicado: {len(idx_buy)} BUY duplicados ‚Üí {len(X_train)} muestras finales")


        # ============================================================================
        # UNDERSAMPLING F√çSICO DESACTIVADO
        # ============================================================================
        # RAZ√ìN: Evitar triple penalizaci√≥n (Undersampling + Class Weights + Focal Loss)
        # Ahora usamos solo AUTO_CLASS_WEIGHTS que balancea autom√°ticamente.
        # 
        # El c√≥digo original reduc√≠a agresivamente HOLD, pero esto causaba:
        # 1. Confusi√≥n en el modelo por se√±ales contradictorias
        # 2. P√©rdida de datos valiosos de mercados laterales
        # 3. Overfitting a movimientos extremos
        # ============================================================================
        
        # C√ìDIGO ORIGINAL COMENTADO:
        # # 5. UNDERSAMPLING F√çSICO (Para eliminar bias de HOLD)
        # print("\n‚öñÔ∏è Aplicando UNDERSAMPLING f√≠sico...")
        # 
        # # Identificar √≠ndices por clase
        # idx_sell = np.where(y_train == 0)[0]
        # idx_hold = np.where(y_train == 1)[0]
        # idx_buy = np.where(y_train == 2)[0]
        # 
        # n_sell = len(idx_sell)
        # n_hold = len(idx_hold)
        # n_buy = len(idx_buy)
        # 
        # print(f"   Original: SELL={n_sell}, HOLD={n_hold}, BUY={n_buy}")
        # 
        # # Determinar tama√±o objetivo (el de la clase minoritaria, o un poco m√°s)
        # # Usamos el promedio de BUY/SELL como target para HOLD
        # target_size = int((n_sell + n_buy) / 2)
        # if target_size < 100: target_size = 100 # Safety
        # 
        # # Si HOLD es mayoritario, recortamos
        # if n_hold > target_size * 1.5: # Solo si hay mucho desbalance
        #     print(f"   ‚úÇÔ∏è Recortando HOLD de {n_hold} a ~{target_size}...")
        #     idx_hold_keep = np.random.choice(idx_hold, size=target_size, replace=False)
        #     
        #     # Combinar √≠ndices
        #     final_indices = np.concatenate([idx_sell, idx_hold_keep, idx_buy])
        #     np.random.shuffle(final_indices)
        #     
        #     X_train = X_train[final_indices]
        #     y_train = y_train[final_indices]
        #     
        #     print(f"   ‚úÖ Dataset Balanceado: {len(X_train)} muestras")
        #     print(f"   Distribuci√≥n: SELL={n_sell}, HOLD={target_size}, BUY={n_buy}")
        # else:
        #     print("   ‚úÖ No se requiere undersampling agresivo.")
        
        # Mostrar distribuci√≥n actual (sin undersampling)
        from collections import Counter
        class_counts = Counter(y_train)
        print("\nüìä Distribuci√≥n de clases (SIN undersampling f√≠sico):")
        print(f"   SELL (0): {class_counts[0]} ({class_counts[0]/len(y_train)*100:.1f}%)")
        print(f"   HOLD (1): {class_counts[1]} ({class_counts[1]/len(y_train)*100:.1f}%)")
        print(f"   BUY (2): {class_counts[2]} ({class_counts[2]/len(y_train)*100:.1f}%)")
        print(f"   Total: {len(y_train)} muestras")
        print(f"   ‚ÑπÔ∏è El balanceo se har√° mediante CLASS_WEIGHTS durante entrenamiento")

        print(f"\n‚úÖ Dataset Final Preparado:")
        print(f"   Train: {len(X_train)} muestras (Mezclado)")
        print(f"   Val:   {len(X_val)} muestras (Ordenado por par)")
        print(f"   Shape: {X_train.shape}")

        # CALCULAR PESOS DE CLASES
        if config.USE_AUTO_CLASS_WEIGHTS:
            # Opci√≥n A: Autom√°tico (sklearn)
            from sklearn.utils import class_weight
            class_weights_vals = class_weight.compute_class_weight(
                class_weight='balanced',
                classes=np.unique(y_train),
                y=y_train
            )
            class_weights_dict = dict(enumerate(class_weights_vals))
            print("\n‚öñÔ∏è Pesos de clases calculados AUTOM√ÅTICAMENTE (Balanced):")
        else:
            # Opci√≥n B: Manual (desde config)
            class_weights_dict = config.CLASS_WEIGHTS
            print("\n‚öñÔ∏è Pesos de clases MANUALES (desde config):")

        # Mostrar pesos usados
        print("\nüìä PESOS DE CLASES APLICADOS:")
        for cls, weight in class_weights_dict.items():
            label = config.CLASS_LABELS[cls]
            count = np.sum(y_train == cls)
            percentage = count / len(y_train) * 100
            print(f"   {label} ({cls}): {count} muestras ({percentage:.1f}%) ‚Üí Peso: {weight:.3f}")
        
        # Calcular ratio de pesos para verificar balance
        weights_array = np.array([class_weights_dict[i] for i in range(config.NUM_CLASSES)])

        if config.USE_BINARY_CLASSIFICATION and config.NUM_CLASSES == 2:
            print(f"\n   Ratio de pesos (NO_BUY:BUY) = {weights_array[0]:.2f}:{weights_array[1]:.2f}")
        elif not config.USE_BINARY_CLASSIFICATION and config.NUM_CLASSES == 3:
            print(f"\n   Ratio de pesos (SELL:HOLD:BUY) = "
                f"{weights_array[0]:.2f}:{weights_array[1]:.2f}:{weights_array[2]:.2f}")
        else:
            print(f"\n   Pesos calculados: {weights_array.tolist()}")
 

        print(f"   ‚ö†Ô∏è IMPORTANTE: BUY debe tener peso ALTO para ser aprendido correctamente")

        
        return X_train, y_train, X_val, y_val, class_weights_dict
    
    def train_initial_model(self, symbols=None, timeframe='4h', start_date=None, end_date=None, epochs=None):
        """Entrena modelo inicial desde cero
        
        Args:
            symbols: Lista de s√≠mbolos
            timeframe: Timeframe
            start_date: Fecha inicio (YYYY-MM-DD) o None para todo el hist√≥rico
            end_date: Fecha fin (YYYY-MM-DD) o None hasta presente
            epochs: N√∫mero de √©pocas (opcional)
        """
        print("\n" + "="*60)
        print("üéì ENTRENAMIENTO INICIAL")
        print("="*60)
        
        # Preparar datos (ahora retorna weights)
        X_train, y_train, X_val, y_val, class_weights = self.prepare_training_data(
            symbols, timeframe, start_date, end_date
        )
        
        # Crear modelo
        input_shape = (X_train.shape[1], X_train.shape[2])  # (lookback, n_features)
        self.model = NeuralTradingModel(input_shape)
        
        # Mostrar resumen
        self.model.get_summary()
        
        # Entrenar (pasando weights y epochs)
        history = self.model.train(X_train, y_train, X_val, y_val, epochs=epochs, class_weights=class_weights)
        
        # Evaluar
        accuracy = self.model.evaluate(X_val, y_val)
        
        # Guardar
        self.current_version += 1
        self.model.save(self.current_version)
        self.feature_extractor.save_scaler(self.current_version)
        
        # Guardar m√©tricas
        self.save_metrics(self.current_version, {
            'accuracy': accuracy,
            'train_samples': len(X_train),
            'val_samples': len(X_val),
            'timestamp': datetime.now().isoformat(),
            'symbols': symbols or config.DEFAULT_SYMBOLS,
            'timeframe': timeframe
        })
        
        print(f"\n‚úÖ Modelo v{self.current_version} entrenado y guardado")
        
        return self.model
    
    def save_metrics(self, version, metrics):
        """Guarda m√©tricas de un modelo"""
        path = Path(config.MODELS_DIR) / config.METRICS_NAME_FORMAT.format(version=version)
        with open(path, 'w') as f:
            json.dump(metrics, f, indent=2)
        print(f"üíæ M√©tricas guardadas: {path}")
    
    def load_metrics(self, version):
        """Carga m√©tricas de un modelo"""
        path = Path(config.MODELS_DIR) / config.METRICS_NAME_FORMAT.format(version=version)
        if path.exists():
            with open(path, 'r') as f:
                return json.load(f)
        return None


class NeuralStrategy:
    """Interfaz ligera para predicci√≥n en tiempo real"""
    
    def __init__(self, version=None, model_name=None):
        """
        Args:
            version: Versi√≥n del modelo a cargar (sistema legacy, None = √∫ltima)
            model_name: Nombre del modelo a cargar (nuevo sistema, None = default)
        """
        self.cache = DataCache()
        self.feature_extractor = FeatureExtractor()
        self.model = None
        self.version = version
        self.model_name = model_name
        self.input_shape = None
        
        # Cargar modelo
        if model_name is not None:
            self.load_model_by_name(model_name)
        else:
            self.load_model(version)
    
    def load_model_by_name(self, name=None):
        """Carga modelo usando ModelManager (nuevo sistema)"""
        from .model_manager import ModelManager
        
        manager = ModelManager()
        result = manager.load_model(name)
        
        if result is None:
            print(f"‚ùå No se pudo cargar modelo '{name}'")
            return False
        
        model, scaler, metadata = result
        self.model = model
        self.feature_extractor.scaler = scaler
        self.model_name = metadata.get('name')
        self.version = None  # Clear version when using named model
        self.input_shape = self.model.input_shape[1:]  # (lookback, features)
        
        print(f"‚úÖ Modelo '{self.model_name}' cargado exitosamente")
        return True
    
    def load_model(self, version=None):
        """Carga modelo y scaler"""
        learner = ContinuousLearner()
        
        if version is None:
            version = learner.get_latest_version()
        
        if version == 0:
            print("‚ùå No hay modelos disponibles. Entrena uno primero:")
            print("   python neural_strategy.py --mode train")
            return False
        
        self.version = version
        
        # Cargar scaler
        if not self.feature_extractor.load_scaler(version):
            return False
        
        # Cargar modelo neuronal
        from tensorflow import keras
        model_path = Path(config.MODELS_DIR) / config.MODEL_NAME_FORMAT.format(version=version)
        
        if not model_path.exists():
            print(f"‚ùå Modelo v{version} no encontrado en {model_path}")
            return False
        
        try:
            self.model = keras.models.load_model(model_path)
            print(f"‚úÖ Modelo v{version} cargado")
        except Exception as e:
            print(f"‚ùå Error cargando modelo: {e}")
            return False
        
        # Obtener input shape del modelo cargado
        self.input_shape = self.model.input_shape[1:]  # (lookback, features)
        
        print(f"‚úÖ Estrategia neuronal v{version} lista")
        return True
    
    def predict_signal(self, X):
        """
        Predice se√±al con etiqueta
        
        Args:
            X: Features (1, lookback, n_features) o (lookback, n_features)
        
        Returns:
            dict: {'signal': 'BUY'/'SELL'/'HOLD', 'confidence': float, 'probabilities': dict}
        """
        # Asegurar shape correcto
        if len(X.shape) == 2:
            X = np.expand_dims(X, axis=0)
        
        # Predicci√≥n
        probs = self.model.predict(X, verbose=0)[0]
        
        # Clase con mayor probabilidad
        predicted_class = np.argmax(probs)
        confidence = probs[predicted_class]
        
        # Aplicar umbrales de confianza
        signal = config.CLASS_LABELS[predicted_class]
        
        if signal == 'BUY' and confidence < config.MIN_CONFIDENCE_BUY:
            signal = 'HOLD'
        elif signal == 'SELL' and confidence < config.MIN_CONFIDENCE_SELL:
            signal = 'HOLD'
        
        # NUEVO: Filtro anti-confusi√≥n BUY/SELL
        # Si predice BUY pero SELL tiene probabilidad alta, es se√±al ambigua ‚Üí HOLD
        # Si predice SELL pero BUY tiene probabilidad alta, es se√±al ambigua ‚Üí HOLD
        if signal == 'BUY' and probs[0] > 0.35:  # SELL prob > 35%
            signal = 'HOLD'
            confidence = probs[1]  # Usar confianza de HOLD
        elif signal == 'SELL' and probs[2] > 0.35:  # BUY prob > 35%
            signal = 'HOLD'
            confidence = probs[1]  # Usar confianza de HOLD
        
        return {
            'signal': signal,
            'confidence': float(confidence),
            'probabilities': {
                'SELL': float(probs[0]),
                'HOLD': float(probs[1]),
                'BUY': float(probs[2])
            }
        }
    
    def get_signal(self, symbol, timeframe='4h'):
        """
        Obtiene se√±al de trading para un s√≠mbolo
        
        MODO PREDICCI√ìN: Solo carga √∫ltimas N velas (eficiente)
        
        Args:
            symbol: Par de trading
            timeframe: Timeframe
        
        Returns:
            dict: {'signal': 'BUY'/'SELL'/'HOLD', 'confidence': float, ...}
        """
        # Cargar solo √∫ltimas velas necesarias
        df = self.cache.get_data(symbol, timeframe)
        
        if df is None or len(df) < config.LOOKBACK_WINDOW:
            return {
                'signal': 'HOLD',
                'confidence': 0.0,
                'error': 'Datos insuficientes'
            }
        
        # Tomar solo √∫ltimas velas
        df_recent = df.tail(config.LOOKBACK_WINDOW + 50)  # +50 para c√°lculo de indicadores
        
        # Extraer features
        X = self.feature_extractor.extract_features(df_recent, fit_scaler=False)
        
        # Crear secuencia (solo √∫ltima)
        X_seq = self.feature_extractor.create_sequences(X)
        
        if len(X_seq) == 0:
            return {
                'signal': 'HOLD',
                'confidence': 0.0,
                'error': 'No se pudieron crear secuencias'
            }
        
        # Predecir √∫ltima secuencia
        X_last = X_seq[-1:]
        
        # Verificar que modelo est√© cargado
        if self.model is None:
            return {
                'signal': 'HOLD',
                'confidence': 0.0,
                'error': 'Modelo no cargado'
            }
        
        # Generar se√±al
        result = self.predict_signal(X_last)
        result['symbol'] = symbol
        result['timestamp'] = datetime.now().isoformat()
        result['version'] = self.version
        
        return result


# ==================== CLI ====================

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Neural Trading Strategy')
    parser.add_argument('--mode', type=str, required=True,
                       choices=['train', 'predict', 'continuous', 'test'],
                       help='Modo de operaci√≥n')
    parser.add_argument('--symbols', nargs='+', default=None,
                       help='S√≠mbolos para entrenar (ej: ETH/USDT BTC/USDT)')
    parser.add_argument('--symbol', type=str, default='ETH/USDT',
                       help='S√≠mbolo para predicci√≥n')
    parser.add_argument('--timeframe', type=str, default='4h',
                       help='Timeframe')
    parser.add_argument('--epochs', type=int, default=None,
                       help='N√∫mero de √©pocas')
    parser.add_argument('--version', type=int, default=None,
                       help='Versi√≥n del modelo')
    parser.add_argument('--name', type=str, default=None,
                       help='Nombre del modelo (ej: BTC_2020_2024)')
    parser.add_argument('--start-date', type=str, default=None,
                       help='Fecha inicio entrenamiento (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, default=None,
                       help='Fecha fin entrenamiento (YYYY-MM-DD)')
    
    args = parser.parse_args()
    
    if args.mode == 'train':
        print("\nüéì MODO: Entrenamiento Inicial\n")
        learner = ContinuousLearner()
        model = learner.train_initial_model(
            symbols=args.symbols, 
            timeframe=args.timeframe, 
            start_date=args.start_date,
            end_date=args.end_date,
            epochs=args.epochs
        )
        
        # Si se especific√≥ un nombre, guardar con ModelManager
        if args.name and model is not None:
            from .model_manager import ModelManager
            manager = ModelManager()
            
            metadata = {
                'symbols': args.symbols or config.DEFAULT_SYMBOLS,
                'timeframe': args.timeframe,
                'start_date': args.start_date or 'all',
                'end_date': args.end_date or 'present',
                'description': f"Model trained on {args.symbols or config.DEFAULT_SYMBOLS}",
            }
            
            if manager.save_model(model.model, learner.feature_extractor.scaler, args.name, metadata):
                print(f"\n‚úÖ Modelo guardado como: {args.name}")
            else:
                print(f"\n‚ö†Ô∏è Error guardando modelo con nombre: {args.name}")
    
    elif args.mode == 'predict':
        print("\n‚ö° MODO: Predicci√≥n\n")
        strategy = NeuralStrategy(args.version)
        result = strategy.get_signal(args.symbol, args.timeframe)
        
        print(f"\n{'='*60}")
        print(f"Se√±al para {result.get('symbol', args.symbol)}")
        print(f"{'='*60}")
        print(f"üìä SE√ëAL: {result['signal']}")
        print(f"üéØ Confianza: {result['confidence']:.2%}")
        print(f"\nüìà Probabilidades:")
        for signal, prob in result['probabilities'].items():
            print(f"   {signal}: {prob:.2%}")
        print(f"{'='*60}\n")
    
    elif args.mode == 'test':
        print("\nüß™ MODO: Test de Features\n")
        cache = DataCache()
        df = cache.get_data(args.symbol, args.timeframe)
        
        print(f"‚úÖ Datos cargados: {len(df)} velas")
        
        fe = FeatureExtractor()
        X = fe.extract_features(df, fit_scaler=True)
        
        print(f"‚úÖ Features extra√≠das: {X.shape}")
        print(f"   Features: {fe.feature_names}")
        
        X_seq = fe.create_sequences(X)
        print(f"‚úÖ Secuencias creadas: {X_seq.shape}")
    
    elif args.mode == 'continuous':
        print("\nüîÑ MODO: Aprendizaje Continuo")
        print("‚ö†Ô∏è No implementado a√∫n")
        print("   Este modo ejecutar√≠a reentrenamiento peri√≥dico")

